{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Overview ===\n",
      "Data shape: (57580, 125)\n",
      "Vocabulary size: 8293\n",
      "\n",
      "=== Special Tokens ===\n",
      "<START>: 8291\n",
      "<EOP>: 8290\n",
      " : Not found\n",
      "\n",
      "=== Sample Characters ===\n",
      "春: 3189\n",
      "江: 193\n",
      "花: 2808\n",
      "月: 6933\n",
      "夜: 7440\n",
      "\n",
      "=== First Few ix2word Mappings ===\n",
      "0: 憁\n",
      "1: 耀\n",
      "2: 枅\n",
      "3: 涉\n",
      "4: 谈\n",
      "5: 伊\n",
      "6: 鈌\n",
      "7: 薙\n",
      "8: 亟\n",
      "9: 洞\n",
      "\n",
      "=== DataLoader Test ===\n",
      "Batch shape: torch.Size([128, 125])\n",
      "\n",
      "=== Single Poem Example ===\n",
      "Structure breakdown:\n",
      "Total length: 125 characters\n",
      "Padding length: 75 </s> tokens\n",
      "Actual poem length: 50 characters\n",
      "\n",
      "Actual poem text:\n",
      "<START>度门能不访，冒雪屡西东。已想人如玉，遥怜马似骢。乍迷金谷路，稍变上阳宫。还比相思意，纷纷正满空。<EOP>\n",
      "\n",
      "First few character mappings (excluding padding):\n",
      "Index 8291 -> Character '<START>'\n",
      "Index 6731 -> Character '度'\n",
      "Index 4770 -> Character '门'\n",
      "Index 1787 -> Character '能'\n",
      "Index 8118 -> Character '不'\n",
      "Index 7577 -> Character '访'\n",
      "Index 7066 -> Character '，'\n",
      "Index 4817 -> Character '冒'\n",
      "Index 648 -> Character '雪'\n",
      "Index 7121 -> Character '屡'\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from data import get_data\n",
    "from main import Config, PoetryModel\n",
    "import numpy as np\n",
    "\n",
    "def test_data_loading():\n",
    "    \"\"\"Test data loading and examine data structures\"\"\"\n",
    "    # Initialize config\n",
    "    opt = Config()\n",
    "    opt.pickle_path = './data/tang.npz'\n",
    "    \n",
    "    # Get data\n",
    "    data, word2ix, ix2word = get_data(opt)\n",
    "    print(\"\\n=== Data Overview ===\")\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Vocabulary size: {len(word2ix)}\")\n",
    "    \n",
    "    # Check special tokens and handle potential missing tokens\n",
    "    print(\"\\n=== Special Tokens ===\")\n",
    "    special_tokens = ['<START>', '<EOP>', ' ']\n",
    "    for token in special_tokens:\n",
    "        try:\n",
    "            print(f\"{token}: {word2ix.get(token, 'Not found')}\")\n",
    "        except:\n",
    "            print(f\"{token}: Not in vocabulary\")\n",
    "    \n",
    "    # Show some sample vocabulary items\n",
    "    print(\"\\n=== Sample Characters ===\")\n",
    "    sample_chars = list(\"春江花月夜\")\n",
    "    for char in sample_chars:\n",
    "        if char in word2ix:\n",
    "            print(f\"{char}: {word2ix[char]}\")\n",
    "    \n",
    "    # Show first few items in ix2word\n",
    "    print(\"\\n=== First Few ix2word Mappings ===\")\n",
    "    for i in range(min(10, len(ix2word))):\n",
    "        print(f\"{i}: {ix2word[i]}\")\n",
    "    \n",
    "    return data, word2ix, ix2word, opt\n",
    "\n",
    "def test_dataloader(data, opt, ix2word):\n",
    "    \"\"\"Test DataLoader functionality\"\"\"\n",
    "    print(\"\\n=== DataLoader Test ===\")\n",
    "    data = t.from_numpy(data)\n",
    "    dataloader = t.utils.data.DataLoader(\n",
    "        data,\n",
    "        batch_size=opt.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=1\n",
    "    )\n",
    "    \n",
    "    # Get one batch\n",
    "    batch = next(iter(dataloader))\n",
    "    print(f\"Batch shape: {batch.shape}\")\n",
    "    \n",
    "    # # Show contents of first few poems in the batch\n",
    "    # print(\"\\n=== Sample Poems from Batch ===\")\n",
    "    # for poem_idx in range(min(3, batch.shape[0])):  # Show first 3 poems\n",
    "    #     poem = batch[poem_idx]\n",
    "    #     # Convert indices to characters and filter out padding\n",
    "    #     chars = [ix2word[idx.item()] for idx in poem if idx.item() < len(ix2word)]\n",
    "    #     print(f\"\\nPoem {poem_idx + 1}:\")\n",
    "    #     print(''.join(chars))\n",
    "    #     print(f\"Length: {len(chars)} characters\")\n",
    "        \n",
    "    #     # Show raw indices for debugging\n",
    "    #     print(\"First 10 indices:\", poem[:10].tolist())\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def test_single_example(data, opt, ix2word):\n",
    "    \"\"\"Show a single example from the dataset with better formatting\"\"\"\n",
    "    print(\"\\n=== Single Poem Example ===\")\n",
    "    \n",
    "    # Convert numpy array to tensor\n",
    "    data = t.from_numpy(data)\n",
    "    single_poem = data[0]\n",
    "    \n",
    "    # Convert indices to characters\n",
    "    chars = [ix2word[idx.item()] for idx in single_poem]\n",
    "    \n",
    "    # Split into padding and actual poem\n",
    "    padding = []\n",
    "    actual_poem = []\n",
    "    for char in chars:\n",
    "        if char == '</s>':\n",
    "            padding.append(char)\n",
    "        else:\n",
    "            actual_poem.append(char)\n",
    "    \n",
    "    print(\"Structure breakdown:\")\n",
    "    print(f\"Total length: {len(chars)} characters\")\n",
    "    print(f\"Padding length: {len(padding)} </s> tokens\")\n",
    "    print(f\"Actual poem length: {len(actual_poem)} characters\")\n",
    "    \n",
    "    print(\"\\nActual poem text:\")\n",
    "    print(''.join(actual_poem))\n",
    "    \n",
    "    # Show first few mappings of the actual poem (skipping padding)\n",
    "    print(\"\\nFirst few character mappings (excluding padding):\")\n",
    "    start_idx = len(padding)  # Skip padding tokens\n",
    "    for i in range(start_idx, min(start_idx + 10, len(chars))):\n",
    "        print(f\"Index {single_poem[i].item()} -> Character '{chars[i]}'\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Run all tests\n",
    "    data, word2ix, ix2word, opt = test_data_loading()\n",
    "    dataloader = test_dataloader(data, opt, ix2word)\n",
    "    test_single_example(data, opt, ix2word)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input indices shape: torch.Size([2, 3])\n",
      "Input indices:\n",
      " tensor([[0, 2, 4],\n",
      "        [1, 3, 5]])\n",
      "\n",
      "Embedding shape: torch.Size([2, 3, 4])\n",
      "Embedded vectors:\n",
      " tensor([[[ 2.1206, -0.2110, -0.0433, -1.3023],\n",
      "         [-1.8514, -0.8825, -0.9430, -0.4287],\n",
      "         [ 0.5552, -0.5138,  0.7194,  0.4082]],\n",
      "\n",
      "        [[ 1.6580,  0.1504, -0.8308,  0.5962],\n",
      "         [ 1.5268, -0.2469, -1.1589,  0.1992],\n",
      "         [-1.5513, -1.4598, -0.0251,  1.0985]]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Embedding for index 0:\n",
      " tensor([ 2.1206, -0.2110, -0.0433, -1.3023], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a small vocabulary (let's say we have 10 Chinese characters)\n",
    "vocab_size = 10\n",
    "embedding_dim = 4  # Small dimension for demonstration\n",
    "batch_size = 3\n",
    "seq_length = 2\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create some sample input indices\n",
    "# Let's say we have 3 sequences, each with 2 characters\n",
    "input_indices = torch.tensor([\n",
    "    [0, 1],  # First sequence: character 0, character 1\n",
    "    [2, 3],  # Second sequence: character 2, character 3\n",
    "    [4, 5]   # Third sequence: character 4, character 5\n",
    "])\n",
    "\n",
    "# Convert input to expected shape [seq_len, batch_size]\n",
    "input_indices = input_indices.transpose(0, 1)\n",
    "\n",
    "print(\"Input indices shape:\", input_indices.shape)\n",
    "print(\"Input indices:\\n\", input_indices)\n",
    "\n",
    "# Get embeddings\n",
    "embedded = embedding(input_indices)\n",
    "\n",
    "print(\"\\nEmbedding shape:\", embedded.shape)\n",
    "print(\"Embedded vectors:\\n\", embedded)\n",
    "\n",
    "# Let's look at one specific embedding\n",
    "print(\"\\nEmbedding for index 0:\\n\", embedded[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape: torch.Size([1, 128, 512])\n",
      "Hidden state (hn) shape: torch.Size([2, 128, 512])\n",
      "Cell state (cn) shape: torch.Size([2, 128, 512])\n",
      "For input character '春':\n",
      "Top 5 predicted next characters for first sequence:\n",
      "Probabilities: tensor([0.0655, 0.0649, 0.0645, 0.0641, 0.0638], grad_fn=<SelectBackward0>)\n",
      "Indices: tensor([2920, 1614, 3569, 4749, 1323])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup example\n",
    "batch_size = 128\n",
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "vocab_size = 5000  # Example vocabulary size\n",
    "\n",
    "# Let's say we have first character '春' (represented by index 42) for all sequences\n",
    "first_char = torch.tensor([[42] * batch_size])  # Shape: [1, 128]\n",
    "\n",
    "# 1. First, embed the character\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "embedded = embedding(first_char)  # Shape: [1, 128, 256]\n",
    "\n",
    "# 2. Initialize LSTM\n",
    "lstm = nn.LSTM(embedding_dim, hidden_size, num_layers)\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "# 3. Process through LSTM\n",
    "output, (hn, cn) = lstm(embedded, (h0, c0))  # hn: final hidden state [num_layers, batch_size, hidden_size]\n",
    "                                            # cn: final cell state [num_layers, batch_size, hidden_size]\n",
    "# Print shapes of LSTM outputs\n",
    "print(\"\\nOutput shape:\", output.shape)  # Should be [1, 128, 512]\n",
    "print(\"Hidden state (hn) shape:\", hn.shape)  # Should be [2, 128, 512] \n",
    "print(\"Cell state (cn) shape:\", cn.shape)  # Should be [2, 128, 512]\n",
    "\n",
    "# 4. Get predictions for next character\n",
    "linear = nn.Linear(hidden_size, vocab_size)\n",
    "predictions = linear(output.squeeze(0))  # Shape: [128, 5000]\n",
    "\n",
    "# 5. Get most likely next characters\n",
    "top_predictions = torch.topk(predictions, k=5, dim=1)\n",
    "\n",
    "print(\"For input character '春':\")\n",
    "print(\"Top 5 predicted next characters for first sequence:\")\n",
    "print(f\"Probabilities: {top_predictions.values[0]}\")\n",
    "print(f\"Indices: {top_predictions.indices[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
